[{"authors":["admin"],"categories":null,"content":"Hi, there! I am from University of Southern California(USC). I obtained by M.S. degree in Electrical Engineering at USC. I am currently working as a research assistant at Plus Lab, a Natural Language Processing group, at USC Information Scienceses Institute(ISI), fortunate to be supervised by Prof. Nanyun (Violet) Peng. I am now appplying for PhD starting from Fall 2020.\nI work on Natural Language Understanding and Informatoin Extraction, specificly Event Temporal Relation Extraction and Biomedical Event Extraction. Previously I also work on Spoken Language Understanding and Speech Synthesis, collaborating with Prof. Panayiotis Georgiou and folks from SCUBA Lab at USC.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://Mu-Y.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi, there! I am from University of Southern California(USC). I obtained by M.S. degree in Electrical Engineering at USC. I am currently working as a research assistant at Plus Lab, a Natural Language Processing group, at USC Information Scienceses Institute(ISI), fortunate to be supervised by Prof. Nanyun (Violet) Peng. I am now appplying for PhD starting from Fall 2020.\nI work on Natural Language Understanding and Informatoin Extraction, specificly Event Temporal Relation Extraction and Biomedical Event Extraction.","tags":null,"title":"Mu Yang","type":"authors"},{"authors":["Rujun Han*","I-Hung Hsu*","**Mu Yang**","Aram Galstyan","Ralph Weischedel","Nanyun Peng"],"categories":[],"content":"","date":1569024407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569024407,"objectID":"5334f3c35534cc87c76f227a8124d603","permalink":"https://Mu-Y.github.io/publication/event_temproal_relation/","publishdate":"2019-09-20T17:06:47-07:00","relpermalink":"/publication/event_temproal_relation/","section":"publication","summary":"","tags":["paper"],"title":"Deep Structured Neural Network for Event Temporal Relation Extraction","type":"publication"},{"authors":["Prashanth Gurunath Shivakumar*","**Mu Yang***","Panayiotis Georgiou"],"categories":[],"content":"","date":1569023786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569023786,"objectID":"cb085a144741b242a3281e59841f13b5","permalink":"https://Mu-Y.github.io/publication/slu_c2v/","publishdate":"2019-09-20T16:56:26-07:00","relpermalink":"/publication/slu_c2v/","section":"publication","summary":"Decoding speakerâ€™s intent is a crucial part of spoken language understanding (SLU). The presence of noise or errors in the text transcriptions, in real life scenarios make the task more challenging. In this paper, we address the spoken language intent detection under noisy conditions imposed by automatic speech recognition (ASR) systems. We propose to employ confusion2vec word feature representation to compensate for the errors made by ASR and to increase the robustness of the SLU system. The confusion2vec, motivated from human speech production and perception, models acoustic relationships between words in addition to the semantic and syntactic relations of words in human language. We hypothesize that ASR often makes errors relating to acoustically similar words, and the confusion2vec with inherent model of acoustic relationships between words is able to compensate for the errors. We demonstrate through experiments on the ATIS benchmark dataset, the robustness of the proposed model to achieve state-of-the-art results under noisy ASR conditions. Our system reduces classification error rate (CER) by 20.84% and improves robustness by 37.48% (lower CER degradation) relative to the previous stateof-the-art going from clean to noisy transcripts. Improvements are also demonstrated when training the intent detection models on noisy transcripts.","tags":["paper"],"title":"Spoken Language Intent Detection using Confusion2Vec","type":"publication"},{"authors":["**Mu Yang**","James Bunning","Shiyu Mou","Sharada Murali","Yixin Yang"],"categories":null,"content":" Method We implemented the Timbre model mentioned in the framework of the NPSS paper. The input acoustic features include log-Mel Spectral Frequency Coefficients(MFSCs) and Band Aperiodicity(AP) which are extracted by WORLD vocoder. The generation is conditioned on control inputs including F0(extracted by WORLD) and time-aligned phonemes. During generation(inference), the model generates MFSCs and APs, which are used for audio re-synthesis along with the true F0 via WORLD vocoder.\nFor more details, take a look at our report and the original NPSS paper.\nDataset We used two datasets: 1) NIT Japanese Nursery dataset and 2) self-curated Coldplay songs dataset.\nResults(Audio Samples) Listen to some of our sythesized audio samples below!\n Trained on NIT data. We took one of the training recordings as target. Resynthesized using true F0, generated MFSC and AP.  Target Synthesized  Trained on NIT data. Generating previously unseen sequences by splicing together random clips from the NIT recordings and doing a similar concatenation of the corresponding F0 and phonemes for each audio clip.  Target Synthesized  Trained on self-created dataset. we resynthesized recordings in the Coldplay dataset using true F0 and AP, and MFSCs generated by the harmonic submodel.  Target Synthesized   ","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"a486509cfd9f048a6f9c798a35be5b8f","permalink":"https://Mu-Y.github.io/publication/synthsing/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/publication/synthsing/","section":"publication","summary":"Final project for USC course EE599: Deep Learning Lab for Speech Processing - a WaveNet-based singing voice synthesizer. This is a partial implement of the paper [A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs](https://www.mdpi.com/2076-3417/7/12/1313).","tags":["proj"],"title":"Synthing: A WaveNet-based Singing Voice Synthisizer","type":"publication"},{"authors":["**Mu Yang**"],"categories":null,"content":" Method Mel-frequency Spectrum features are used to train the Auto-encoder Network.\nFor more details, take a look at the GitHub page.\nDataset Audio downloaded from YouTube. Sox is used for audio pre-processing(e.g. downsampling and trimming)\nResults(Audio Samples) Listen to some of our sythesized audio samples below!\n Trained on NIT data. We took one of the training recordings as target. Resynthesized using true F0, generated MFSC and AP.  Target Synthesized  Trained on NIT data. Generating previously unseen sequences by splicing together random clips from the NIT recordings and doing a similar concatenation of the corresponding F0 and phonemes for each audio clip.  Target Synthesized  Trained on self-created dataset. we resynthesized recordings in the Coldplay dataset using true F0 and AP, and MFSCs generated by the harmonic submodel.  Target Synthesized   ","date":1538697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538697600,"objectID":"b7dee0ab30b0460c6177573e859f0df2","permalink":"https://Mu-Y.github.io/publication/auto-encoder-audio-denoiser/","publishdate":"2018-10-05T00:00:00Z","relpermalink":"/publication/auto-encoder-audio-denoiser/","section":"publication","summary":"An Auto-Encoder Neural Network based Audio Denoiser. It trys to reconstruct the clean audio from the noise-mixed audio. Mel-frequency Spectrum features are used to train the Neural Network.","tags":["proj"],"title":"AutoEncoder-Audio-Denoiser","type":"publication"}]