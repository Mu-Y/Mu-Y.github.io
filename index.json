[{"authors":["admin"],"categories":null,"content":"Hi, there! My name is Mu Yang. I\u0026rsquo;m a 1st year Ph.D. student in Electrical and Computer Engineering (ECE) at University of Texas at Dallas. My supervisor is Prof. John H. L. Hansen. I\u0026rsquo;m a member of UTD Center for Robust Speech Systems (UTD CRSS). My research interests include Speech Recognition and Speech Synthesis. In the past, I also had experience in Natural Language Processing, especially in Information Extraction (Event and Event Temporal Relation Extraction).\nI obtained my M.S. degree in Electrical Engineering at University of Southern California (USC). During my Master\u0026rsquo;s study, I worked with Dr. Panayiotis Georgiou on Spoken Language Understanding and Speech Synthesis problems. I also worked with Dr. Nanyun (Violet) Peng on Natural Langauge Processing and Information Extraction at USC Information Scienceses Institute (USC ISI). During 2020-2021, I was a Computer Science Ph.D. student at Texas A\u0026amp;M Univeristy, working on Speech Mis-pronunciation Recognition and Speech Synthesis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://Mu-Y.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi, there! My name is Mu Yang. I\u0026rsquo;m a 1st year Ph.D. student in Electrical and Computer Engineering (ECE) at University of Texas at Dallas. My supervisor is Prof. John H. L. Hansen. I\u0026rsquo;m a member of UTD Center for Robust Speech Systems (UTD CRSS). My research interests include Speech Recognition and Speech Synthesis. In the past, I also had experience in Natural Language Processing, especially in Information Extraction (Event and Event Temporal Relation Extraction).","tags":null,"title":"Mu Yang","type":"authors"},{"authors":["**Mu Yang**","Shaojin Ding","Tianlong Chen","Tong Wang","Zhangyang Wang"],"categories":[],"content":"","date":1633823786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633823786,"objectID":"c4afe5049bf028bd4c24063553147319","permalink":"https://Mu-Y.github.io/publication/lll_tts/","publishdate":"2021-10-09T16:56:26-07:00","relpermalink":"/publication/lll_tts/","section":"publication","summary":"This work presents a lifelong learning approach to train a multilingual Text-To-Speech (TTS) system, where each language was seen as an individual task and was learned sequentially and continually. It does not require pooled data from all languages altogether, and thus alleviates the storage and computation burden. One of the challenges of lifelong learning methods is \"catastrophic forgetting\": in TTS scenario it means that model performance quickly degrades on previous languages when adapted to a new language. We approach this problem via a data-replay-based lifelong learning method. We formulate the replay process as a supervised learning problem, and propose a simple yet effective dual-sampler framework to tackle the heavily language-imbalanced training samples. Through objective and subjective evaluations, we show that this supervised learning formulation outperforms other gradient-based and regularization-based lifelong learning methods, achieving 43% Mel-Cepstral Distortion reduction compared to a fine-tuning baseline.","tags":["paper"],"title":"Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis","type":"publication"},{"authors":["Mingyu Derek Ma*","Jiao Sun*","**Mu Yang**","Kung-Hsiang Huang","Nuan Wen","Shikhar Singh","Rujun Han","Nanyun Peng"],"categories":[],"content":"","date":1610582807,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610582807,"objectID":"8ff3ec6a66eaf219b0a9a5c0c100bea6","permalink":"https://Mu-Y.github.io/publication/eventplus/","publishdate":"2021-01-13T17:06:47-07:00","relpermalink":"/publication/eventplus/","section":"publication","summary":"We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications.","tags":["paper"],"title":"EventPlus: A Temporal Event Understanding Pipeline","type":"publication"},{"authors":["**Mu Yang**","Karolina Nurzynska","Ann E. Walts","Arkadiusz Gertych"],"categories":[],"content":"","date":1608336407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336407,"objectID":"f68c69afef825e1b3117f8e66b66a7ef","permalink":"https://Mu-Y.github.io/publication/cnn_tb/","publishdate":"2020-12-18T17:06:47-07:00","relpermalink":"/publication/cnn_tb/","section":"publication","summary":"Tuberculosis is the most common mycobacterial disease that affects humans worldwide. Rapid and reliable diagnosis of mycobacteria is crucial to identify infected individuals, to initiate and monitor treatment and to minimize or prevent transmission. Microscopic identification of acid-fast mycobacteria (AFB) in tissue sections is usually accomplished by examining Ziehl-Neelsen (ZN) stained slides in which AFB appear bright red against the blue background.\nBecause the ZN-stained slides require time consuming and meticulous screening by an experienced pathologist, our team developed a machine learning pipeline to classify digitized ZN-stained slides as AFB-positive or AFB-negative. The pipeline includes two convolutional neural network (CNN) models to recognize tiles containing AFB, and a logistic regression (LR) model to classify slides based on features from AFB-probability maps assembled from the CNN tile-based classification results. The first CNN was trained using tiles from 6 AFB-positive and 8 AFB-negative slides, and the second CNN was trained using the initial tile set expanded by additional tiles from 19 AFB-negative slides selected within an active learning framework. When evaluated on a separate set of tiles, the two CNNs yielded F1 scores of 99.03% and 98.75%, respectively, and were used to classify tiles in a separate set of 134 slides (46 AFB-positive and 88 AFB-negative). The classification yielded two AFB-probability maps, one for each CNN. The LR model was then 10-fold cross-validated using the average of feature vectors extracted from the AFB-probability maps generated by each CNN. The feature vector consisted of seven features of the AFB-probability map histogram and the positive tile rate (PTR). The sensitivity (87.13%), specificity (87.62%) and F1 (80.18%) achieved by this model were superior to the baseline performance of PTR-based separation of slides that yielded F1 scores of 73.13% and 66.67% in the AFB-probability maps outputted by the CNN trained within the active learning framework and the CNN trained only on the initial set of slides, respectively.\nOur CNNs outperformed several recently published models for AFB detection. Active learning induced robust learning of features by the CNN and led to improved LR classification performance of slides. In the 52 AFB-positive slides used in the pipeline development, the AFB were infrequent, predominantly single and only rarely found in small clusters. Our pipeline can classify slides and visualize suspected AFB-positive areas in each slide, and thus potentially facilitate evaluation of ZN-stained tissue sections for AFB.","tags":["paper"],"title":"A CNN-based active learning framework to identify mycobacteria in digitized Ziehl-Neelsen stained human tissues","type":"publication"},{"authors":["Kung-Hsiang Huang","**Mu Yang**","Nanyun Peng"],"categories":[],"content":"","date":1605917207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605917207,"objectID":"113b80d1bcdcc875e5c3855b03cfb5ac","permalink":"https://Mu-Y.github.io/publication/biomed_event_extraction_gnn/","publishdate":"2020-11-20T17:06:47-07:00","relpermalink":"/publication/biomed_event_extraction_gnn/","section":"publication","summary":"Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus. One of the main challenges is to identify nested structured events that are associated with non-indicative trigger words. We propose to incorporate domain knowledge from Unified Medical Language System (UMLS) to a pre-trained language model via a hierarchical graph representation encoded by a proposed Graph Edgeconditioned Attention Networks (GEANet). To better recognize the trigger words, each sentence is first grounded to a sentence graph based on a jointly modeled hierarchical knowledge graph from UMLS. The grounded graphs are then propagated by GEANet, a novel graph neural networks for enhanced capabilities in inferring complex events. On BioNLP 2011 GENIA Event Extraction task, our approach achieved 1.41% F1 and 3.19% F1 improvements on all events and complex events, respectively. Ablation studies confirm the importance of GEANet and hierarchical KG.","tags":["paper"],"title":"Biomedical Event Extraction with Hierarchical Knowledge Graphs","type":"publication"},{"authors":["Rujun Han*","I-Hung Hsu*","**Mu Yang**","Aram Galstyan","Ralph Weischedel","Nanyun Peng"],"categories":[],"content":"","date":1569024407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569024407,"objectID":"5334f3c35534cc87c76f227a8124d603","permalink":"https://Mu-Y.github.io/publication/event_temproal_relation/","publishdate":"2019-09-20T17:06:47-07:00","relpermalink":"/publication/event_temproal_relation/","section":"publication","summary":"We propose a novel deep structured learning framework for event temporal relation extraction. The model consists of 1) a recurrent neural network (RNN) to learn scoring functions for pair-wise relations, and 2) a structured support vector machine (SSVM) to make joint predictions. The neural network automatically learns representations that account for long-term contexts to provide robust features for the structured model, while the SSVM incorporates domain knowledge such as transitive closure of temporal relations as constraints to make better globally consistent decisions. By jointly training the two components, our model combines the benefits of both data-driven learning and knowledge exploitation. Experimental results on three high-quality event temporal relation datasets (TCR, MATRES, and TB-Dense) demonstrate that incorporated with pre-trained contextualized embeddings, the proposed model achieves significantly better performances than the state-of-the-art methods on all three datasets. We also provide thorough ablation studies to investigate our model.","tags":["paper"],"title":"Deep Structured Neural Network for Event Temporal Relation Extraction","type":"publication"},{"authors":["Prashanth Gurunath Shivakumar*","**Mu Yang***","Panayiotis Georgiou"],"categories":[],"content":"","date":1569023786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569023786,"objectID":"cb085a144741b242a3281e59841f13b5","permalink":"https://Mu-Y.github.io/publication/slu_c2v/","publishdate":"2019-09-20T16:56:26-07:00","relpermalink":"/publication/slu_c2v/","section":"publication","summary":"Decoding speaker’s intent is a crucial part of spoken language understanding (SLU). The presence of noise or errors in the text transcriptions, in real life scenarios make the task more challenging. In this paper, we address the spoken language intent detection under noisy conditions imposed by automatic speech recognition (ASR) systems. We propose to employ confusion2vec word feature representation to compensate for the errors made by ASR and to increase the robustness of the SLU system. The confusion2vec, motivated from human speech production and perception, models acoustic relationships between words in addition to the semantic and syntactic relations of words in human language. We hypothesize that ASR often makes errors relating to acoustically similar words, and the confusion2vec with inherent model of acoustic relationships between words is able to compensate for the errors. We demonstrate through experiments on the ATIS benchmark dataset, the robustness of the proposed model to achieve state-of-the-art results under noisy ASR conditions. Our system reduces classification error rate (CER) by 20.84% and improves robustness by 37.48% (lower CER degradation) relative to the previous stateof-the-art going from clean to noisy transcripts. Improvements are also demonstrated when training the intent detection models on noisy transcripts.","tags":["paper"],"title":"Spoken Language Intent Detection using Confusion2Vec","type":"publication"},{"authors":["**Mu Yang**","James Bunning","Shiyu Mou","Sharada Murali","Yixin Yang"],"categories":null,"content":" Method We implemented the Timbre model mentioned in the framework of the NPSS paper.\nUnlike the vanilla sample-to-sample WaveNet, the proposed model makes frame-to-frame predictions on 60-dimensional log-Mel Spectral Frequency Coefficients(MFSCs)and 4-dimensional Band Aperiodicity(AP) Coefficients, using F0(coarse coded), phoneme identity(one-hot coded) and normalized phoneme position(coarse coded) as local control inputs and singer identity as global control inputs. Then we feed generated MFSCs and APs, as well as true F0 into WORLD vocoder to synthesize audio. The features, i.e. MFSCs, APs and F0 are also extracted via WORLD.\nFor more details, take a look at our report and the original NPSS paper.\nDataset We used two datasets: 1) NIT Japanese Nursery dataset and 2) self-curated Coldplay songs dataset.\nResults(Audio Samples) Listen to some of our sythesized audio samples below!\n Trained on NIT data. We took one of the training recordings as target. Resynthesized using true F0, generated MFSC and AP.  Target Synthesized  Trained on NIT data. Generating previously unseen sequences by splicing together random clips from the NIT recordings and doing a similar concatenation of the corresponding F0 and phonemes for each audio clip.  Target Synthesized  Trained on self-created dataset. we resynthesized recordings in the Coldplay dataset using true F0 and AP, and MFSCs generated by the harmonic submodel.  Target Synthesized   ","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"a486509cfd9f048a6f9c798a35be5b8f","permalink":"https://Mu-Y.github.io/publication/synthsing/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/publication/synthsing/","section":"publication","summary":"Final project for USC course EE599: Deep Learning Lab for Speech Processing - a WaveNet-based singing voice synthesizer. This is a partial implementation of the paper [A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs](https://www.mdpi.com/2076-3417/7/12/1313).","tags":["proj"],"title":"Synthing: A WaveNet-based Singing Voice Synthisizer","type":"publication"},{"authors":["**Mu Yang**","Tao Chen","Chang Su","Zhe Yang"],"categories":null,"content":" Method Lyrics Collection We start from a list of artists. We utilize the iTunes Search API to request meta data for each artist, including song name, genre, album/collection name, release year, etc. Then a web crawler is applied to fetch lyrics on Genius, using artist names and song names. Finally the collected lyrics are cleaned up.\nClassifiers We applied the following classifiers to compare their performances:\n KNN, with TF-IDF as features Naive Bayes, with TF-IDF as features SVM, with TF-IDF as features LSTM, with Glove word embedding as features  For more details, take a look at our report.\nResults Lyrics Collection: We manage to obtain a lyrics-genre dataset, with 30649 lyrics from 8 genres.\nClassifiers: We train the classifiers on a balanced set - 390 lyrics for each genre, from which we hold out a balanced set as test set. Accuracy on the held-out test set is presented below.\n   Model Accuracy     KNN 0.426   Naive Bayes 0.597   SVM 0.588   LSTM 0.563    Interestingly, the Baive Bayes Classifier works pretty well, outperforming all others. That said, I have to recognize, the LSTM model has a few hyperparameters(e.g. hidden layer dimension, learning rate, etc) to be tuned. But due to the time and computation resource limitation, these tuning are not adequately performed. It\u0026rsquo;ll be interesting to see in the future that whether there will be more performance boost with a thorough parameter tuning.\n","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"1bd361dfbffb1dbb300d4c743cf683d2","permalink":"https://Mu-Y.github.io/publication/lyrics_classification/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/publication/lyrics_classification/","section":"publication","summary":"Web crawler of lyrics and corresponding music genre. Multiple baseline classifiers, such as Naive Bayes, SVM and Neural Approach(LSTM) are applied to identify the genre of a song by analyzing its lyrics.","tags":["proj"],"title":"Collection and Classification of Lyrics","type":"publication"},{"authors":["**Mu Yang**"],"categories":null,"content":" Method Mel-frequency Spectrum features are used to train the Auto-encoder Network.\nFor more details, take a look at the GitHub page.\nDataset Audio downloaded from YouTube. Sox is used for audio pre-processing(e.g. downsampling and trimming)\nResults(Audio Samples) Listen to some of the audio samples below!\n Training audio  Original Noise-mixed De-noised  Testing audio  Original [Noise-mixed(unavailable due to copyright issue)] De-noised   ","date":1538697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538697600,"objectID":"b7dee0ab30b0460c6177573e859f0df2","permalink":"https://Mu-Y.github.io/publication/auto-encoder-audio-denoiser/","publishdate":"2018-10-05T00:00:00Z","relpermalink":"/publication/auto-encoder-audio-denoiser/","section":"publication","summary":"An Auto-Encoder Neural Network based Audio Denoiser. It trys to reconstruct the clean audio from the noise-mixed audio. Mel-frequency Spectrum features are used to train the Neural Network.","tags":["proj"],"title":"Auto-Encoder-based Audio Denoiser","type":"publication"},{"authors":["**Mu Yang**"],"categories":null,"content":" Method Mostly referred to Balázs Bank, 2008\nDataset Open-source Room Impulse Response from openairlib.net(Unfortunately, this amazing website is currently down due to unknown reasons.). The file r8-omni-conf_b.wav in the GitHub repo is one of the recorded Room Impulse Response wav files from a sound studio of the Laboratory of Music Acoustics Technology (LabMAT) at the Department of Music Studies of the University of Athens.\nResults(Audio Samples) Convolve audio with the Room Impulse Responses, to hear the impact of Room Impulse Response on the listening experience, and also the calibration of the equalizer.\n Sound studio Impulse Response(data accompanying this repo), audio is a classical music clip.\n Original: audio Conv with Room Impulse Response: audio Conv with Equalized Room Impulse Response: audio  Another Impulse Response from an office room, audio is an EDM clip.\n Original: audio Conv with Room Impulse Response: audio Conv with Equalized Room Impulse Response: audio   ","date":1524182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524182400,"objectID":"d24406556236a25a86c61c9520e00a93","permalink":"https://Mu-Y.github.io/publication/roomir-equalizer/","publishdate":"2018-04-20T00:00:00Z","relpermalink":"/publication/roomir-equalizer/","section":"publication","summary":"A Parallel second-order-based equalizer for Room Impulse Response Calibration.","tags":["proj"],"title":"Digital Room Correction using Parallel Second-order Filter-based Equalizer","type":"publication"},{"authors":["**Mu Yang**"],"categories":null,"content":" Method Re-train Faster-RCNN on VOC 2007 and VOC 2012, and also Caltech pedestrian dataset to perform pedestrian detection.\nApply the model on videos to generate pedestrian bounding-boxes frame-by-frame, with some bounding boxes smoothing techniques.\nResults When fed with a video where pedestrians appear during specific frames, the network will process and output a new video with pedestrians marked by the bouding boxes.\n","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"3eadc25bfeafeb7eb6dd438c1b9c1785","permalink":"https://Mu-Y.github.io/publication/faster_rcnn/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/faster_rcnn/","section":"publication","summary":"Train and deploy a Faster-RCNN framework to perform pedestrain detection in videos.","tags":["proj"],"title":"Faster-RCNN for Pedestrian Detection in Videos","type":"publication"}]