<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications | Mu Yang&#39;s Website</title>
    <link>https://Mu-Y.github.io/publication/</link>
      <atom:link href="https://Mu-Y.github.io/publication/index.xml" rel="self" type="application/rss+xml" />
    <description>Publications</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 20 Sep 2019 17:06:47 -0700</lastBuildDate>
    <image>
      <url>https://Mu-Y.github.io/img/icon-192.png</url>
      <title>Publications</title>
      <link>https://Mu-Y.github.io/publication/</link>
    </image>
    
    <item>
      <title>Deep Structured Neural Network for Event Temporal Relation Extraction</title>
      <link>https://Mu-Y.github.io/publication/event_temproal_relation/</link>
      <pubDate>Fri, 20 Sep 2019 17:06:47 -0700</pubDate>
      <guid>https://Mu-Y.github.io/publication/event_temproal_relation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spoken Language Intent Detection using Confusion2Vec</title>
      <link>https://Mu-Y.github.io/publication/slu_c2v/</link>
      <pubDate>Fri, 20 Sep 2019 16:56:26 -0700</pubDate>
      <guid>https://Mu-Y.github.io/publication/slu_c2v/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Synthing: A WaveNet-based Singing Voice Synthisizer</title>
      <link>https://Mu-Y.github.io/publication/synthsing/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://Mu-Y.github.io/publication/synthsing/</guid>
      <description>

&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;

&lt;p&gt;We implemented the Timbre model mentioned in the framework of the &lt;a href=&#34;https://www.mdpi.com/2076-3417/7/12/1313&#34; target=&#34;_blank&#34;&gt;NPSS&lt;/a&gt; paper. The input acoustic features include log-Mel Spectral Frequency Coefficients(&lt;strong&gt;MFSCs&lt;/strong&gt;) and Band Aperiodicity(&lt;strong&gt;AP&lt;/strong&gt;) which are extracted by WORLD vocoder. The generation is &lt;strong&gt;conditioned on control inputs&lt;/strong&gt; including &lt;strong&gt;F0&lt;/strong&gt;(extracted by WORLD) and time-aligned &lt;strong&gt;phonemes&lt;/strong&gt;. During generation(inference), the model generates MFSCs and APs, which are used for audio re-synthesis along with the true F0 via WORLD vocoder.&lt;/p&gt;

&lt;p&gt;For more details, take a look at our &lt;a href=&#34;https://github.com/Mu-Y/SynthSing/blob/master/Final_report.pdf&#34; target=&#34;_blank&#34;&gt;report&lt;/a&gt; and the original &lt;a href=&#34;https://www.mdpi.com/2076-3417/7/12/1313&#34; target=&#34;_blank&#34;&gt;NPSS paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;We used two datasets: 1) NIT Japanese Nursery dataset and 2) self-curated Coldplay songs dataset.&lt;/p&gt;

&lt;h3 id=&#34;results-audio-samples&#34;&gt;Results(Audio Samples)&lt;/h3&gt;

&lt;p&gt;Listen to some of our sythesized audio samples below!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trained on NIT data. We took one of the training recordings as target. Resynthesized using true F0, generated MFSC and AP.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit-004_orignal?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_004_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Trained on NIT data. Generating previously unseen sequences by splicing together random clips from the NIT recordings and doing a similar concatenation of the corresponding F0 and phonemes for each audio clip.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_scramble_original?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_scramble_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Trained on self-created dataset. we resynthesized recordings in the Coldplay dataset using true F0 and AP, and MFSCs generated by the harmonic submodel.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/coldplay-song02-01-007?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/coldplay_007_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AutoEncoder-Audio-Denoiser</title>
      <link>https://Mu-Y.github.io/publication/auto-encoder-audio-denoiser/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://Mu-Y.github.io/publication/auto-encoder-audio-denoiser/</guid>
      <description>

&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;

&lt;p&gt;Mel-frequency Spectrum features are used to train the Auto-encoder Network.&lt;/p&gt;

&lt;p&gt;For more details, take a look at the &lt;a href=&#34;https://github.com/Mu-Y/SynthSing/blob/master/Final_report.pdf&#34; target=&#34;_blank&#34;&gt;GitHub page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Audio downloaded from YouTube. Sox is used for audio pre-processing(e.g. downsampling and trimming)&lt;/p&gt;

&lt;h3 id=&#34;results-audio-samples&#34;&gt;Results(Audio Samples)&lt;/h3&gt;

&lt;p&gt;Listen to some of our sythesized audio samples below!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trained on NIT data. We took one of the training recordings as target. Resynthesized using true F0, generated MFSC and AP.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit-004_orignal?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_004_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Trained on NIT data. Generating previously unseen sequences by splicing together random clips from the NIT recordings and doing a similar concatenation of the corresponding F0 and phonemes for each audio clip.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_scramble_original?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_scramble_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Trained on self-created dataset. we resynthesized recordings in the Coldplay dataset using true F0 and AP, and MFSCs generated by the harmonic submodel.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/coldplay-song02-01-007?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/coldplay_007_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
