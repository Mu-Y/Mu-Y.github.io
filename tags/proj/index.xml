<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>proj | Mu Yang&#39;s Website</title>
    <link>https://Mu-Y.github.io/tags/proj/</link>
      <atom:link href="https://Mu-Y.github.io/tags/proj/index.xml" rel="self" type="application/rss+xml" />
    <description>proj</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 05 Dec 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://Mu-Y.github.io/img/icon-192.png</url>
      <title>proj</title>
      <link>https://Mu-Y.github.io/tags/proj/</link>
    </image>
    
    <item>
      <title>Synthing: A WaveNet-based Singing Voice Synthisizer</title>
      <link>https://Mu-Y.github.io/publication/synthsing/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://Mu-Y.github.io/publication/synthsing/</guid>
      <description>

&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;

&lt;p&gt;We implemented the Timbre model mentioned in the framework of the &lt;a href=&#34;https://www.mdpi.com/2076-3417/7/12/1313&#34; target=&#34;_blank&#34;&gt;NPSS&lt;/a&gt; paper.&lt;/p&gt;

&lt;p&gt;Unlike the vanilla sample-to-sample WaveNet, the proposed model makes frame-to-frame predictions on 60-dimensional log-Mel Spectral Frequency Coefficients(&lt;strong&gt;MFSCs&lt;/strong&gt;)and 4-dimensional Band Aperiodicity(&lt;strong&gt;AP&lt;/strong&gt;) Coefficients, using F0(coarse coded), phoneme identity(one-hot coded) and normalized phoneme position(coarse coded) as local control inputs and singer identity as global control inputs. Then we feed generated MFSCs and APs, as well as true F0 into WORLD vocoder to synthesize audio. The features, i.e. MFSCs, APs and F0 are also extracted via WORLD.&lt;/p&gt;

&lt;p&gt;For more details, take a look at our &lt;a href=&#34;https://github.com/Mu-Y/SynthSing/blob/master/Final_report.pdf&#34; target=&#34;_blank&#34;&gt;report&lt;/a&gt; and the original &lt;a href=&#34;https://www.mdpi.com/2076-3417/7/12/1313&#34; target=&#34;_blank&#34;&gt;NPSS paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;We used two datasets: 1) NIT Japanese Nursery dataset and 2) self-curated Coldplay songs dataset.&lt;/p&gt;

&lt;h3 id=&#34;results-audio-samples&#34;&gt;Results(Audio Samples)&lt;/h3&gt;

&lt;p&gt;Listen to some of our sythesized audio samples below!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trained on NIT data. We took one of the training recordings as target. Resynthesized using true F0, generated MFSC and AP.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit-004_orignal?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_004_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Trained on NIT data. Generating previously unseen sequences by splicing together random clips from the NIT recordings and doing a similar concatenation of the corresponding F0 and phonemes for each audio clip.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_scramble_original?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/hit_scramble_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Trained on self-created dataset. we resynthesized recordings in the Coldplay dataset using true F0 and AP, and MFSCs generated by the harmonic submodel.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/coldplay-song02-01-007?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/coldplay_007_synthesized?in=mu-yang-974011976/sets/results-for-synthsing&#34; target=&#34;_blank&#34;&gt;Synthesized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Collection and Classification of Lyrics</title>
      <link>https://Mu-Y.github.io/publication/lyrics_classification/</link>
      <pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://Mu-Y.github.io/publication/lyrics_classification/</guid>
      <description>

&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;

&lt;h4 id=&#34;lyrics-collection&#34;&gt;Lyrics Collection&lt;/h4&gt;

&lt;p&gt;We start from a list of artists. We utilize the &lt;a href=&#34;https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/index.html&#34; target=&#34;_blank&#34;&gt;iTunes Search API&lt;/a&gt; to request meta data for each artist, including song name, genre, album/collection name, release year, etc. Then a web crawler is applied to fetch lyrics on &lt;a href=&#34;https://genius.com/&#34; target=&#34;_blank&#34;&gt;Genius&lt;/a&gt;, using artist names and song names. Finally the collected lyrics are cleaned up.&lt;/p&gt;

&lt;h4 id=&#34;classifiers&#34;&gt;Classifiers&lt;/h4&gt;

&lt;p&gt;We applied the following classifiers to compare their performances:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;KNN, with TF-IDF as features&lt;/li&gt;
&lt;li&gt;Naive Bayes, with TF-IDF as features&lt;/li&gt;
&lt;li&gt;SVM, with TF-IDF as features&lt;/li&gt;
&lt;li&gt;LSTM, with Glove word embedding as features&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details, take a look at our &lt;a href=&#34;https://github.com/Mu-Y/CSCI544_Prj/blob/master/final_report.pdf&#34; target=&#34;_blank&#34;&gt;report&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Lyrics Collection:&lt;/strong&gt; We manage to obtain a lyrics-genre dataset, with 30649 lyrics from 8 genres.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Classifiers:&lt;/strong&gt; We train the classifiers on a balanced set - 390 lyrics for each genre, from which we hold out a balanced set as test set. Accuracy on the held-out test set is presented below.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;KNN&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.426&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Naive Bayes&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.597&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.588&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;LSTM&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.563&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Interestingly, the Baive Bayes Classifier works pretty well, outperforming all others. That said, I have to recognize, the LSTM model has a few hyperparameters(e.g. hidden layer dimension, learning rate, etc) to be tuned. But due to the time and computation resource limitation, these tunning is not adequantly performed. It&amp;rsquo;ll be interesting to see in the future that whether there will be more performance boost with a thorough parameter tuning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Auto-Encoder-based Audio Denoiser</title>
      <link>https://Mu-Y.github.io/publication/auto-encoder-audio-denoiser/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://Mu-Y.github.io/publication/auto-encoder-audio-denoiser/</guid>
      <description>

&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;

&lt;p&gt;Mel-frequency Spectrum features are used to train the Auto-encoder Network.&lt;/p&gt;

&lt;p&gt;For more details, take a look at the &lt;a href=&#34;https://github.com/Mu-Y/SynthSing/blob/master/Final_report.pdf&#34; target=&#34;_blank&#34;&gt;GitHub page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Audio downloaded from YouTube. Sox is used for audio pre-processing(e.g. downsampling and trimming)&lt;/p&gt;

&lt;h3 id=&#34;results-audio-samples&#34;&gt;Results(Audio Samples)&lt;/h3&gt;

&lt;p&gt;Listen to some of the audio samples below!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Training audio

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/s_train?in=mu-yang-974011976/sets/auto-encoder-audio-denoiser&#34; target=&#34;_blank&#34;&gt;Original&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/x_train?in=mu-yang-974011976/sets/auto-encoder-audio-denoiser&#34; target=&#34;_blank&#34;&gt;Noise-mixed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/d_train?in=mu-yang-974011976/sets/auto-encoder-audio-denoiser&#34; target=&#34;_blank&#34;&gt;De-noised&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Testing audio

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/s_test?in=mu-yang-974011976/sets/auto-encoder-audio-denoiser&#34; target=&#34;_blank&#34;&gt;Original&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Noise-mixed(failed due to copyright issue)]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/d_test?in=mu-yang-974011976/sets/auto-encoder-audio-denoiser&#34; target=&#34;_blank&#34;&gt;De-noised&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Parallel Second-order Filter-based Equalizer for Digital Room Correction</title>
      <link>https://Mu-Y.github.io/publication/roomir-equalizer/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://Mu-Y.github.io/publication/roomir-equalizer/</guid>
      <description>

&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;

&lt;p&gt;Mostly referred to &lt;a href=&#34;https://ieeexplore.ieee.org/document/4529229&#34; target=&#34;_blank&#34;&gt;Bal√°zs Bank, 2008&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Open-source Room Impulse Response from &lt;a href=&#34;http://www.openairlib.net/auralizationdb/content/live-room-sound-studio-laboratory-university-athens&#34; target=&#34;_blank&#34;&gt;openairlib.net&lt;/a&gt;(Unfortunately, this amazing website is currently down due to unknown reasons.). The file &lt;code&gt;r8-omni-conf_b.wav&lt;/code&gt; in the &lt;a href=&#34;https://github.com/Mu-Y/RoomIR-equalizer.git&#34; target=&#34;_blank&#34;&gt;GitHub repo&lt;/a&gt; is one of the recorded Room Impulse Response wav files from a sound studio of the Laboratory of Music Acoustics Technology (LabMAT) at the Department of Music Studies of the University of Athens.&lt;/p&gt;

&lt;h3 id=&#34;results-audio-samples&#34;&gt;Results(Audio Samples)&lt;/h3&gt;

&lt;p&gt;Convolve audio with the Room Impulse Responses, to hear the impact of Room Impulse Response on the listening experience, and also the calibration of the equalizer.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sound studio Impulse Response(data accompanying this repo), audio is a classical music clip.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Original: &lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/classic-short?in=mu-yang-974011976/sets/roomir-equalizer&#34; target=&#34;_blank&#34;&gt;audio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conv with Room Impulse Response: &lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/classic-short-live-studio?in=mu-yang-974011976/sets/roomir-equalizer&#34; target=&#34;_blank&#34;&gt;audio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conv with &lt;strong&gt;Equalized&lt;/strong&gt; Room Impulse Response: &lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/classic-short-live-studio-eq?in=mu-yang-974011976/sets/roomir-equalizer&#34; target=&#34;_blank&#34;&gt;audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Another Impulse Response from an office room, audio is an EDM clip.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Original: &lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/edm-short?in=mu-yang-974011976/sets/roomir-equalizer&#34; target=&#34;_blank&#34;&gt;audio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conv with Room Impulse Response: &lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/edm-short-air-office?in=mu-yang-974011976/sets/roomir-equalizer&#34; target=&#34;_blank&#34;&gt;audio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conv with &lt;strong&gt;Equalized&lt;/strong&gt; Room Impulse Response: &lt;a href=&#34;https://soundcloud.com/mu-yang-974011976/edm-short-air-office-eq?in=mu-yang-974011976/sets/roomir-equalizer&#34; target=&#34;_blank&#34;&gt;audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
