<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shiyu Mou | Mu Yang&#39;s Website</title>
    <link>https://Mu-Y.github.io/authors/shiyu-mou/</link>
      <atom:link href="https://Mu-Y.github.io/authors/shiyu-mou/index.xml" rel="self" type="application/rss+xml" />
    <description>Shiyu Mou</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 05 Dec 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://Mu-Y.github.io/img/icon-192.png</url>
      <title>Shiyu Mou</title>
      <link>https://Mu-Y.github.io/authors/shiyu-mou/</link>
    </image>
    
    <item>
      <title>Synthing: A WaveNet-based Singing Voice Synthisizer</title>
      <link>https://Mu-Y.github.io/publication/synthsing/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://Mu-Y.github.io/publication/synthsing/</guid>
      <description>

&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;

&lt;p&gt;We implemented the Timbre model mentioned in the framework of the &lt;a href=&#34;https://www.mdpi.com/2076-3417/7/12/1313&#34; target=&#34;_blank&#34;&gt;NPSS&lt;/a&gt; paper.&lt;/p&gt;

&lt;p&gt;Unlike the vanilla sample-to-sample WaveNet, the proposed model makes frame-to-frame predictions on 60-dimensional log-Mel Spectral Frequency Coefficients(&lt;strong&gt;MFSCs&lt;/strong&gt;)and 4-dimensional Band Aperiodicity(&lt;strong&gt;AP&lt;/strong&gt;) Coefficients, using F0(coarse coded), phoneme identity(one-hot coded) and normalized phoneme position(coarse coded) as local control inputs and singer identity as global control inputs. Then we feed generated MFSCs and APs, as well as true F0 into WORLD vocoder to synthesize audio. The features, i.e. MFSCs, APs and F0 are also extracted via WORLD.&lt;/p&gt;

&lt;p&gt;For more details, take a look at our &lt;a href=&#34;https://github.com/Mu-Y/SynthSing/blob/master/Final_report.pdf&#34; target=&#34;_blank&#34;&gt;report&lt;/a&gt; and the original &lt;a href=&#34;https://www.mdpi.com/2076-3417/7/12/1313&#34; target=&#34;_blank&#34;&gt;NPSS paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;We used two datasets: 1) NIT Japanese Nursery dataset (&lt;a href=&#34;http://hts.sp.nitech.ac.jp/archives/2.3/HTSdemo_NIT-SONG070-F001.tar.bz2&#34; target=&#34;_blank&#34;&gt;NIT-SONG070-F001&lt;/a&gt;) and 2) self-curated Coldplay songs dataset.&lt;/p&gt;

&lt;h3 id=&#34;results-audio-samples&#34;&gt;Results (Audio Samples)&lt;/h3&gt;

&lt;p&gt;Listen to some of our sythesized audio samples below!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://mu-y.github.io/speech_samples/synthsing/&#34; target=&#34;_blank&#34;&gt;Audio samples html page&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
